{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver_win32/chromedriver')\n",
    "# 암묵적으로 웹 자원 로드를 위해 3초까지 기다려 준다.\n",
    "driver.implicitly_wait(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.naver.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방영종료예능검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_fu():\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    page=soup.select('span.page_number > strong.total > em._total')\n",
    "    p=int(page[0].text.strip())\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_name('query').send_keys('방영종료예능') #검색\n",
    "driver.find_element_by_xpath('//*[@id=\"search_btn\"]').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프로그램명 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/a').click()\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[1]/a').click()  #2018년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[2]/a').click()  #2017년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[3]/a').click()  #2016년\n",
    "driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[4]/a').click()  #2015년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[5]/a').click()  #2014년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[6]/a').click()  #2013년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[7]/a').click()  #2012년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[8]/a').click()  #2011년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[9]/a').click()  #2010년\n",
    "title_list=[]\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "pn=page_fu()\n",
    "for n in range(0,pn):\n",
    "    driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[3]/div/a[2]').click() #다음페이지\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #time.sleep(1)\n",
    "    title=soup.select('div.item > dl.bd_detatil > dt.bd_title > a')\n",
    "    if len(title)==0:\n",
    "        title_list.append('')\n",
    "    elif len(title)!=0:\n",
    "        for i in title:\n",
    "            t=i.text.strip()\n",
    "            title_list.append(t)\n",
    "            \n",
    "            \n",
    "print(len(title_list))  #진행보고서 데이터 개수랑 확인 필수!(2018년의 경우, 증가될 수 있음/확인필수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방영일자(방영종료예능 검색시 나오는) 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/a').click()\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[1]/a').click()  #2018년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[2]/a').click()  #2017년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[3]/a').click()  #2016년\n",
    "driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[4]/a').click()  #2015년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[5]/a').click()  #2014년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[6]/a').click()  #2013년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[7]/a').click()  #2012년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[8]/a').click()  #2011년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[9]/a').click()  #2010년\n",
    "date_list=[]\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "pn=page_fu()\n",
    "for n in range(0,pn):\n",
    "    driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[3]/div/a[2]').click() #다음페이지\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #time.sleep(1)\n",
    "    date=soup.select('tr > td > div > dl > dd.start_day > span')\n",
    "    if len(date)==0:\n",
    "        date_list.append('')\n",
    "    elif len(date)!=0:\n",
    "        for i in date:\n",
    "            d=i.text.strip()\n",
    "            date_list.append(d)\n",
    "    \n",
    "    \n",
    "print(len(date_list))  # title 수와 동일한지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방송사(방영종료예능 검색시 나오는) 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/a').click()\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[1]/a').click()  #2018년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[2]/a').click()  #2017년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[3]/a').click()  #2016년\n",
    "driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[4]/a').click()  #2015년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[5]/a').click()  #2014년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[6]/a').click()  #2013년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[7]/a').click()  #2012년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[8]/a').click()  #2011년\n",
    "#driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[1]/div[2]/div[1]/div/div/ul/li[9]/a').click()  #2010년\n",
    "broad_list=[]\n",
    "broad_list2=[] #프로그램이 2개인 경우\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "pn=page_fu()\n",
    "for n in range(0,pn):\n",
    "    driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[1]/div[2]/div[3]/div/a[2]').click() #다음페이지\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #time.sleep(1)\n",
    "    broad=soup.select('div.item > dl.bd_detatil > dd.bd_info')\n",
    "    if len(broad)==0:\n",
    "        broad_list.append('')\n",
    "    elif len(broad)!=0:\n",
    "        for i in broad:\n",
    "            b=i.text.strip()\n",
    "            c=b.splitlines()\n",
    "            if len(c)==1:\n",
    "                broad_list.append(c[0])\n",
    "                broad_list2.append('')\n",
    "            elif len(c)!=1:\n",
    "                broad_list.append(c[0].replace(\", \",\"\"))\n",
    "                broad_list2.append(c[1].replace(\" \",\"\"))\n",
    "\n",
    "                \n",
    "print(len(broad_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=pd.DataFrame(data={'title_list':title_list,'date_list':date_list})  #데이터프레임 만들기\n",
    "total.to_excel('title_2015.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=pd.DataFrame(data={'broad_list':broad_list,'broad_list2':broad_list2})  #데이터프레임 만들기\n",
    "total.to_excel('broad_2015.xlsx')\n",
    "# 방송사가 2개인 경우가 있기 때문에 구글에 검색 후, 방송사 1개로 추리고 title_2018 파일에 옮겨 저장후->파일 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## daum 방영일자 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(\"title_2014.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list=data['title_list']\n",
    "title_list=list(title_list)\n",
    "date_list=data['date_list']\n",
    "date_list=list(date_list)\n",
    "\n",
    "print(len(title_list))\n",
    "print(len(date_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "day3_list=[]\n",
    "for i in title_list:\n",
    "    driver.get('https://www.daum.net/')\n",
    "    driver.find_element_by_name('q').send_keys(i) #검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"daumSearch\"]/fieldset/div/div/button[2]').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #time.sleep(1)\n",
    "    day=soup.select('span.txt_summary')\n",
    "    if len(day)==0:\n",
    "        day3_list.append('')\n",
    "    elif len(day)!=0:\n",
    "        if len(day)<3:\n",
    "            day3_list.append('') #회차,날짜\n",
    "        elif len(day)>2:\n",
    "            day3=day[2].text.strip()\n",
    "            day3_list.append(day3) #회차,날짜\n",
    "            \n",
    "print(len(day3_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naver 방영일자 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_list=[]  #naver\n",
    "for i in title_list:\n",
    "    driver.get('https://www.naver.com/')\n",
    "    driver.find_element_by_name('query').send_keys(i) #검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"search_btn\"]').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #time.sleep(1)\n",
    "    day=soup.select('div.brcs_detail > dl > dd > span.inline')\n",
    "    if len(day)==0:\n",
    "        day_list.append('')\n",
    "    elif len(day)!=0:\n",
    "        dayy=day[0].text.strip()\n",
    "        day_list.append(dayy)\n",
    "        \n",
    "print(len(day_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=pd.DataFrame(data={'title_list':title_list,'date_list':date_list,'day3_list':day3_list,'day_list':day_list})  #데이터프레임 만들기\n",
    "total.to_excel('date_2015.xlsx')\n",
    "# 방영일자가 다른 경우, 확인 후 title파일에서 수정 및 제거(진행보고서에 작성/제거만)->방영일자가 조금 달라도 프로그램 정보창이 뜨면 ok\n",
    "# 우측 작은 창 X => 커~~~다란 창 검색하자마자 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 1회 시청률 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(\"title_2015.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=data['title_list']\n",
    "title=list(title)\n",
    "\n",
    "len(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent1_list=[]\n",
    "for i in title:\n",
    "    driver.get('https://www.naver.com/')\n",
    "    driver.find_element_by_name('query').send_keys(i+' 1회') #검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"search_btn\"]').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    time.sleep(1)\n",
    "    percent=soup.select('dd.bdcast_rate._multiLayerContainer > span.inline > em.fred')\n",
    "    if len(percent)==0:\n",
    "        percent1_list.append('')\n",
    "    elif len(percent)!=0:\n",
    "        per=percent[0].text.strip()\n",
    "        percent1_list.append(per)\n",
    "        \n",
    "\n",
    "print(len(percent1_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다음 1회 시청률 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent2_list=[]\n",
    "for i in title:\n",
    "    driver.get('https://www.daum.net/')\n",
    "    driver.find_element_by_name('q').send_keys(i+' 1회') #검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"daumSearch\"]/fieldset/div/div/button[2]').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    time.sleep(1)\n",
    "    percent=soup.select('div.episode_info > div.tit_episode > span.txt_rating > span.f_red')\n",
    "    if len(percent)==0:\n",
    "        percent2_list.append('')\n",
    "    elif len(percent)!=0:\n",
    "        per=percent[0].text.strip()\n",
    "        percent2_list.append(per)\n",
    "        \n",
    "        \n",
    "print(len(percent2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=pd.DataFrame(data={'percent1_list':percent1_list,'percent2_list':percent2_list})  #데이터프레임 만들기\n",
    "total.to_excel('percent_2015.xlsx')\n",
    "# percent1_list, percent2_list 하나의 열로 통합 -> 엑셀(=if(percent1='',perent2,percent1)) -> title 파일에 붙이기\n",
    "# 시청률이 없는 경우, 직접 검색한 후 찾아보기/그래도 없으면 공백유지\n",
    "# title_2018 파일에 복붙 후 제거/제거 수 진행보고서 적기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방송종류 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(\"title_2015.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=data['title_list']\n",
    "title=list(title)\n",
    "len(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "day1_list=[]  #daum\n",
    "for i in title:\n",
    "    driver.get('https://www.daum.net/')\n",
    "    driver.find_element_by_name('q').send_keys(i) #검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"daumSearch\"]/fieldset/div/div/button[2]').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #time.sleep(1)\n",
    "    day=soup.select('span.txt_summary')\n",
    "    if len(day)==0:\n",
    "        day1_list.append('')\n",
    "    elif len(day)!=0:\n",
    "        if len(day)<3:\n",
    "            day1=day[0].text.strip()\n",
    "            day1_list.append(day1) #종류\n",
    "        elif len(day)>2:\n",
    "            day1=day[0].text.strip()\n",
    "            day1_list.append(day1) #종류\n",
    "            \n",
    "            \n",
    "print(len(day1_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방송요일, 시간 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_list=[]\n",
    "for i in title:\n",
    "    driver.get('https://www.naver.com/')\n",
    "    driver.find_element_by_name('query').send_keys(i+' 편성시간') #검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"search_btn\"]').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #time.sleep(1)\n",
    "    ex=soup.select('div.group_basic > dl.bf_info > dd.bf_dsc > dl > dd')\n",
    "    if len(ex)==0:\n",
    "        time_list.append('')\n",
    "    elif len(ex)!=0:\n",
    "        time=ex[0].text.strip()\n",
    "        time_list.append(time)\n",
    "        \n",
    "        \n",
    "print(len(time_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방송관람가 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_list=[]\n",
    "for i in title:\n",
    "    driver.get('https://www.naver.com/')\n",
    "    driver.find_element_by_name('query').send_keys(i) #검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"search_btn\"]').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #time.sleep(1)\n",
    "    age=soup.select('div.brcs_box > div.brcs_detail > div.info_bar')\n",
    "    if len(age)==0:\n",
    "        age_list.append('')\n",
    "    elif len(age)!=0:\n",
    "        original=age[0].text.strip()\n",
    "        age_list.append(original)\n",
    "        \n",
    "print(len(age_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사회자 유무 및 사회자 이름 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_name_list=[]\n",
    "for i in title:\n",
    "    driver.get('https://www.naver.com/')\n",
    "    driver.find_element_by_name('query').send_keys(i) #검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"search_btn\"]').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #time.sleep(1)\n",
    "    ex=soup.select('span.menu')\n",
    "    if len(ex)!=0:\n",
    "        if ex[0].text.strip()=='출연진':\n",
    "            driver.find_element_by_xpath('//*[@id=\"bc_program_info\"]/div/div[2]/div[1]/div[1]/div/ul/li[1]/a/span').click()\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            mc=soup.select('div > ul > li > span > a')\n",
    "            if len(mc)==0:\n",
    "                mc_name_list.append('')\n",
    "            elif len(mc)!=0:\n",
    "                mc_del=mc[0].text.strip()\n",
    "                mc_name_list.append(mc_del)\n",
    "        elif ex[0].text.strip()!='출연진':\n",
    "            mc_name_list.append('')\n",
    "    elif len(ex)==0:\n",
    "        mc_name_list.append('')\n",
    "        \n",
    "\n",
    "print(len(mc_name_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PD 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.daum.net/')\n",
    "driver.find_element_by_name('q').send_keys(\"윤식당\") #검색\n",
    "driver.find_element_by_xpath('//*[@id=\"daumSearch\"]/fieldset/div/div/button[2]').click()\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "#time.sleep(1)\n",
    "pd1=soup.select('div > dl > dd > a') #링크O\n",
    "pd2=soup.select('span.gtit')  #링크X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd1_list=[]\n",
    "pd2_list=[]\n",
    "for i in title:\n",
    "    driver.get('https://www.daum.net/')\n",
    "    driver.find_element_by_name('q').send_keys(i) #검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"daumSearch\"]/fieldset/div/div/button[2]').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #time.sleep(1)\n",
    "    pd1=soup.select('div > dl > dd > a') #링크O\n",
    "    pd2=soup.select('span.gtit')  #링크X\n",
    "    if len(pd1)==0: #링크가 없는 경우\n",
    "        if len(pd2)==0:\n",
    "            pd2_list.append('')\n",
    "            pd1_list.append('')\n",
    "        elif len(pd2)!=0:\n",
    "            pd3=pd2[0].text.strip()\n",
    "            pd2_list.append(pd3)\n",
    "            pd1_list.append('')\n",
    "    elif len(pd1)!=0: #링크가 있는 경우\n",
    "        if len(pd2)==0:\n",
    "            pd6=pd1[0].text.strip()\n",
    "            if len(pd6)<5:\n",
    "                pd1_list.append(pd6)\n",
    "                pd2_list.append('')\n",
    "            elif len(pd6)>4:\n",
    "                pd2_list.append('')\n",
    "                pd1_list.append('')\n",
    "        elif len(pd2)!=0:\n",
    "            pd4=pd1[0].text.strip()\n",
    "            pd5=pd2[0].text.strip()\n",
    "            if len(pd4)<5:\n",
    "                pd1_list.append(pd4)\n",
    "                pd2_list.append(pd5)\n",
    "            elif len(pd4)>4:\n",
    "                pd1_list.append('')\n",
    "                pd2_list.append(pd5)\n",
    "        \n",
    "    #링크가 있는 경우, pd1 pd2 둘다 출력(pd1은 감독, pd2 )\n",
    "    #링크가 없는 경우, pd1 X, pd2만\n",
    "    \n",
    "    \n",
    "print(len(pd1_list))\n",
    "print(len(pd2_list))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=pd.DataFrame(data={'day1_list':day1_list,'age_list':age_list,'time_list':time_list,\n",
    "                         'mc_name_list':mc_name_list})\n",
    "total.to_excel('var_2014.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=pd.DataFrame(data={'day1_list':day1_list,'age_list':age_list,'time_list':time_list,\n",
    "                         'mc_name_list':mc_name_list,'pd1_list':pd1_list,\n",
    "                         'pd2_list':pd2_list})\n",
    "total.to_excel('var_2013.xlsx')\n",
    "# pd1, pd2를 pd_list로 통합 후\n",
    "# age_list, 숫자로 변경(일단은)\n",
    "# time_list(요일/시간)을 요일과 시간 2개로 분리 후, title 파일에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사회자 종류 분류 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(\"title_2013.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=data['title_list']\n",
    "title=list(title)\n",
    "\n",
    "mc=data['mc_name_list']\n",
    "mc=list(mc)\n",
    "\n",
    "print(len(mc))\n",
    "print(len(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_list=[]\n",
    "for i in title:\n",
    "    driver.get('https://www.naver.com/')\n",
    "    driver.find_element_by_name('query').send_keys(i) #검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"search_btn\"]').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    ex=soup.select('span.menu')\n",
    "    if len(ex)==0:\n",
    "        ca_list.append('없음')\n",
    "    else:\n",
    "        if ex[0].text.strip()=='출연진':\n",
    "            driver.find_element_by_xpath('//*[@id=\"bc_program_info\"]/div/div[2]/div[1]/div[1]/div/ul/li[1]/a/span').click()\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            exxx=soup.select('span.fblue')\n",
    "            if len(exxx)>0:\n",
    "                ca_list.append('검색필요')\n",
    "            elif len(exxx)==0:\n",
    "                driver.find_element_by_xpath('//*[@id=\"bc_program_info\"]/div/div[2]/div[2]/div[1]/ul/li[1]/span/a').click()\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                exx=soup.select('dd.name > span')\n",
    "                if len(exx)==1:\n",
    "                    ca=exx[0].text.strip()\n",
    "                    ca_list.append(ca)\n",
    "                else:\n",
    "                    ca2=exx[1].text.strip()  #예명을 쓰는 경우,\n",
    "                    ca_list.append(ca2)\n",
    "        else:\n",
    "            ca_list.append('없음')\n",
    "            \n",
    "            \n",
    "print(len(ca_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사회자 수상내역 수 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aw_list=[]\n",
    "cate_list=[]\n",
    "for i in mc:\n",
    "    driver.get('https://people.search.naver.com/')\n",
    "    driver.find_element_by_name('query').send_keys(i) #검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"search_form\"]/fieldset/input').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    exxx=soup.select('div > div > div > div > div > div > div > a > strong')\n",
    "    if len(exxx)==0:  #검색결과가 없는 것\n",
    "        aw_list.append('')\n",
    "        cate_list.append('')\n",
    "    elif len(exxx)!=0:\n",
    "        driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[2]/div[2]/div[1]/div[1]/div/div/a/strong').click()\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        ex=soup.select('div > div > ul.record_tab')  #수상내역이 있는지?\n",
    "        if len(ex)!=0:\n",
    "            e=[]  #버리는 리스트\n",
    "            ex2=ex[0].text.strip()\n",
    "            if '수상내역' in ex2:\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                cate=soup.select('div > div > div > dl > dd.sub')\n",
    "                award=soup.select('div > div > div.record')\n",
    "                y=cate[0].text.strip()\n",
    "                cate_list.append(y)\n",
    "                for k in award:\n",
    "                    j=k.text.strip()\n",
    "                    if '수상내역' in j:\n",
    "                        aw_list.append(j)\n",
    "                    else:\n",
    "                        e.append('')\n",
    "            else:\n",
    "                driver.get('https://people.search.naver.com/')\n",
    "                driver.find_element_by_name('query').send_keys(i) #검색\n",
    "                driver.find_element_by_xpath('//*[@id=\"search_form\"]/fieldset/input').click()\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                exxx=soup.select('div > div > div > div > div > div > div > a > strong')\n",
    "                if len(exxx)==1:   #동명이인이 존재 X\n",
    "                    aw_list.append('없음')\n",
    "                    cate=soup.select('div > div > div > div > div > div > div > span.sub')\n",
    "                    y=cate[0].text.strip()\n",
    "                    cate_list.append(y)\n",
    "                else:\n",
    "                    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[2]/div[2]/div[1]/div[2]/div/div/a/strong').click()\n",
    "                    html = driver.page_source\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    ex=soup.select('div > div > ul.record_tab')  #수상내역이 있는지?\n",
    "                    cate=soup.select('div > div > div > dl > dd.sub')\n",
    "                    y=cate[0].text.strip()\n",
    "                    cate_list.append(y)\n",
    "                    ex2=ex[0].text.strip()\n",
    "                    if '수상내역' in ex2:\n",
    "                        award=soup.select('div > div > div.record')\n",
    "                        l=[]  #버리는 리스트\n",
    "                        for k in award:\n",
    "                            j=k.text.strip()\n",
    "                            if '수상내역' in j:\n",
    "                                aw_list.append(j)\n",
    "                            else:\n",
    "                                l.append('')\n",
    "                    else:\n",
    "                        aw_list.append('없음')\n",
    "        else:\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            cate=soup.select('div > div > div > dl > dd.sub')\n",
    "            y=cate[0].text.strip()\n",
    "            cate_list.append(y)\n",
    "            aw_list.append('')\n",
    "            \n",
    "            \n",
    "print(len(cate_list))\n",
    "print(len(aw_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사회자 수상내역 데이터 전처리\n",
    "award_list=[]\n",
    "year=2013  #본인 파일 연도\n",
    "for i in aw_list:\n",
    "    award=[]\n",
    "    x=str(i)\n",
    "    num=re.findall('\\d+',x)\n",
    "    for j in num:\n",
    "        y=int(j)\n",
    "        if year > y > 1000:\n",
    "            award.append(y)\n",
    "    b=len(award)\n",
    "    award_list.append(b)\n",
    "print(award_list)\n",
    "print(len(award_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사회자 수상내역 데이터 전처리\n",
    "award_list=[]\n",
    "year=2013  #본인 파일 연도\n",
    "for i in aw_list:\n",
    "    award=[]\n",
    "    x=str(i)\n",
    "    num=re.findall('\\d+',x)\n",
    "    for j in num:\n",
    "        y=int(j)\n",
    "        if year > y > 1000:\n",
    "            award.append(y)\n",
    "    b=len(award)\n",
    "    award_list.append(b)\n",
    "print(award_list)\n",
    "print(len(award_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=pd.DataFrame(data={'mc':mc,'ca_list':ca_list,'cate_list':cate_list,'award_list':award_list})\n",
    "total.to_excel('var_2013(2).xlsx')\n",
    "# ca_list랑 cate_list 비교 -> 다르면 ca_list를 기준으로 수상내역 노가다 -> ca_list에 검색필요값 노가다(유형.수상내역)\n",
    "# award_list 수 첫번째랑 마지막만 비교해보기(aw_list)\n",
    "# 완료 후, title에 옮기세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naver pd이름 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(\"title_2013.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=data['title_list']\n",
    "title=list(title)\n",
    "len(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd1_list=[]  \n",
    "for i in title:\n",
    "    driver.get('https://www.naver.com/')\n",
    "    driver.find_element_by_name('query').send_keys(i) #검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"search_btn\"]').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    pd1=soup.select('div.brcs_detail > dl > dd > span')\n",
    "    pd1_list.append(pd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naver_list=[]\n",
    "a1=[]\n",
    "a2=[]\n",
    "for i in range(0, len(pd1_list)):\n",
    "    a = pd1_list[i]\n",
    "    if len(a) == 5:\n",
    "        a1=a[2].text.strip()\n",
    "        if '연출' in a1:\n",
    "            naver_list.append(a1)\n",
    "        else:\n",
    "            naver_list.append('없음')\n",
    "    elif len(a) ==4:\n",
    "        a1=a[1].text.strip()\n",
    "        a2=a[2].text.strip()\n",
    "        if '연출' in a2:\n",
    "            naver_list.append(a2)\n",
    "        elif '연출' in a1:\n",
    "            naver_list.append(a1)\n",
    "        else:\n",
    "            naver_list.append('없음')\n",
    "    elif len(a) == 3:\n",
    "        a1=a[1].text.strip()\n",
    "        if '연출' in a1:\n",
    "            naver_list.append(a1)\n",
    "        else:\n",
    "            naver_list.append('없음')\n",
    "    elif len(a)==2:\n",
    "        a1=a[1].text.strip()\n",
    "        if '연출' in a1:\n",
    "            naver_list.append(a1)\n",
    "        else:\n",
    "            naver_list.append('없음')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## daum pd이름 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daum_list=[]\n",
    "for i in title:\n",
    "    driver.get('https://www.daum.net/')\n",
    "    driver.find_element_by_name('q').send_keys(i) #검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"daumSearch\"]/fieldset/div/div/button[2]').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    pd=soup.select('dl.dl_comm.dl_row')\n",
    "    if len(pd)==0:\n",
    "        daum_list.append('없음')\n",
    "    else:\n",
    "        ex=[]\n",
    "        for n in pd:\n",
    "            x=n.text.strip()\n",
    "            if '제작' in x:\n",
    "                daum_list.append(x)\n",
    "            else:\n",
    "                ex.append('없음')\n",
    "        if len(ex)==len(pd):  #앞에 if가 실행되면 이건 상관x\n",
    "            daum_list.append('없음')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추가적으로 필요할시 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del daum_list[13]   #추가적으로 뽑힌 데이터 삭제\n",
    "#제작이라는 글자가 들어가있는 건 다 끌어오기 때문에 len이 안 맞을 시, 체크 후 제거\n",
    "#python 리스트는 [0,1,2,3, ...]이기 때문에 순서 잘 생각해서 지우기!!\n",
    "#지워버리면 답도 없어서 다시 뽑아야함.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(daum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(daum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=pd.DataFrame(data={'title':title,'daum_list':daum_list,'naver_list':naver_list})\n",
    "total.to_excel('pd_2013.xlsx')\n",
    "# pd_list생성 후, if(daum_list=\"없음\",naver_list,daum_list)형식으로 변수 생성\n",
    "# daum_list 기준으로 pd_list 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAVER pd 포털사이트 검색 유무 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(\"title_2013.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=data['title_list']\n",
    "title=list(title)\n",
    "head(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd=data['pd_list']\n",
    "pd=list(pd)\n",
    "head(pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naver_list=[]\n",
    "for i in pd:\n",
    "    driver.get('https://www.naver.com/')\n",
    "    driver.find_element_by_name('query').send_keys(i+' pd') #검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"search_btn\"]').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    ox=soup.select('dl.detail_profile > dd.name > a > strong')\n",
    "    if len(ox)!=0:\n",
    "        ans='있음'\n",
    "        naver_list.append(ans)\n",
    "    elif len(ox)==0:\n",
    "        ans= '없음'\n",
    "        naver_list.append(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAUM pd 포털사이트 검색 유무 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daum_list=[]\n",
    "for i in pd:\n",
    "    driver.get('https://www.daum.net/')\n",
    "    driver.find_element_by_name('q').send_keys(i+' pd') #검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"daumSearch\"]/fieldset/div/div/button[2]').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    ox=soup.select('a.tit_main > b')\n",
    "    if len(ox)!=0:\n",
    "        ans='있음'\n",
    "        daum_list.append(ans)\n",
    "    elif len(ox)==0:\n",
    "        ans= '없음'\n",
    "        daum_list.append(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PD 수상내역 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aw_list=[]\n",
    "cate_list=[]\n",
    "for i in pd:\n",
    "    driver.get('https://people.search.naver.com/')\n",
    "    driver.find_element_by_name('query').send_keys(i+' pd') #검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"search_form\"]/fieldset/input').click()\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    exxx=soup.select('div > div > div > div > div > div > div > a > strong')\n",
    "    if len(exxx)==0:  #검색결과가 없는 것\n",
    "        aw_list.append('')\n",
    "        cate_list.append('')\n",
    "    elif len(exxx)!=0:\n",
    "        driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[2]/div[2]/div[1]/div[1]/div/div/a/strong').click()\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        ex=soup.select('div > div > ul.record_tab')  #수상내역이 있는지?\n",
    "        if len(ex)!=0:\n",
    "            e=[]  #버리는 리스트\n",
    "            ex2=ex[0].text.strip()\n",
    "            if '수상내역' in ex2:\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                cate=soup.select('div > div > div > dl > dd.sub')\n",
    "                award=soup.select('div > div > div.record')\n",
    "                y=cate[0].text.strip()\n",
    "                cate_list.append(y)\n",
    "                for k in award:\n",
    "                    j=k.text.strip()\n",
    "                    if '수상내역' in j:\n",
    "                        aw_list.append(j)\n",
    "                    else:\n",
    "                        e.append('')\n",
    "            else:\n",
    "                driver.get('https://people.search.naver.com/')\n",
    "                driver.find_element_by_name('query').send_keys(i) #검색\n",
    "                driver.find_element_by_xpath('//*[@id=\"search_form\"]/fieldset/input').click()\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                exxx=soup.select('div > div > div > div > div > div > div > a > strong')\n",
    "                if len(exxx)==1:   #동명이인이 존재 X\n",
    "                    aw_list.append('없음')\n",
    "                    cate=soup.select('div > div > div > div > div > div > div > span.sub')\n",
    "                    y=cate[0].text.strip()\n",
    "                    cate_list.append(y)\n",
    "                else:\n",
    "                    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[2]/div[2]/div[1]/div[2]/div/div/a/strong').click()\n",
    "                    html = driver.page_source\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    ex=soup.select('div > div > ul.record_tab')  #수상내역이 있는지?\n",
    "                    cate=soup.select('div > div > div > dl > dd.sub')\n",
    "                    y=cate[0].text.strip()\n",
    "                    cate_list.append(y)\n",
    "                    ex2=ex[0].text.strip()\n",
    "                    if '수상내역' in ex2:\n",
    "                        award=soup.select('div > div > div.record')\n",
    "                        l=[]  #버리는 리스트\n",
    "                        for k in award:\n",
    "                            j=k.text.strip()\n",
    "                            if '수상내역' in j:\n",
    "                                aw_list.append(j)\n",
    "                            else:\n",
    "                                l.append('')\n",
    "                    else:\n",
    "                        aw_list.append('없음')\n",
    "        else:\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            cate=soup.select('div > div > div > dl > dd.sub')\n",
    "            y=cate[0].text.strip()\n",
    "            cate_list.append(y)\n",
    "            aw_list.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cate_list))\n",
    "print(len(aw_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=pd.DataFrame(data={'title':title,'pd':pd,'naver_list':naver_list,'daum_list':daum_list,'pd_award_list':pd_award_list})  #데이터프레임 만들기\n",
    "total.to_excel('pdox_2013.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
